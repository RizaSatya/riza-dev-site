[{"content":"Introduction Let\u0026rsquo;s face it, pushing new code to production can be nerve-wracking. One tiny bug and suddenly you\u0026rsquo;re putting out fires instead of sipping your well-earned coffee. That\u0026rsquo;s where canary deployments come in handy. They let you dip your toes in the water before diving in headfirst. In this article, we\u0026rsquo;re going to walk through setting up a canary deployment system on Google Cloud Platform (GCP) using Terraform. We\u0026rsquo;ll be working with regional managed instance groups, load balancers, and some traffic-splitting magic to get a solid canary setup going.\nWhile continuous deployment tools like Chef or Ansible are often part of a complete canary strategy, they\u0026rsquo;re outside the scope of what will be discussed here. The goal is to share insights on creating a robust infrastructure base that can support canary deployments, which can then be integrated with various deployment tools and processes.\nWhat is a Canary Deployment Canary deployment is a way to safely roll out new software versions. Instead of updating everything at once, you release the new version to a small group of users first. This lets you test the waters and catch any problems early. If everything looks good, you gradually increase the number of users getting the new version.\nLet\u0026rsquo;s take a look at the following analogy, imagine you\u0026rsquo;re a chef introducing a new recipe at your restaurant. Instead of immediately replacing your popular dish with this new recipe for all customers, you decide to take a cautious approach:\nYou prepare a small batch of the new recipe alongside your usual menu. When customers order, most get the original dish, but a small percentage (let\u0026rsquo;s say 10%) receive the new recipe. You closely watch for feedback. Are the customers who got the new dish enjoying it? Are there any complaints or allergic reactions? If the new recipe is well-received, you gradually increase the number of customers who get it, maybe to 25%, then 50%, and so on. If there are issues with the new recipe, you can quickly stop serving it and revert to the original dish for all customers, minimizing the impact. Once you\u0026rsquo;re confident the new recipe is a hit, you fully replace the old dish with the new one. This approach is essentially what a canary deployment does in software:\nThe original dish is your stable version of the software. The new recipe is your updated version. The gradual rollout to a small percentage of customers is how you introduce the new version to a subset of users. Monitoring feedback is like watching for errors or performance issues in your application. The ability to quickly revert is your safety net if something goes wrong. Just as this approach lets a chef test a new recipe without risking the satisfaction of all customers, a canary deployment allows developers to test new software versions with minimal risk to the overall user base.\nArchitecture Diagram Components HTTP Load Balancer\nForwarding Rule\nresource \u0026#34;google_compute_forwarding_rule\u0026#34; \u0026#34;default\u0026#34; { name = \u0026#34;http-lb-forwarding-rule\u0026#34; region = \u0026#34;us-central1\u0026#34; target = google_compute_region_target_http_proxy.default.id port_range = \u0026#34;80\u0026#34; load_balancing_scheme = \u0026#34;INTERNAL_MANAGED\u0026#34; network = \u0026#34;default\u0026#34; subnetwork = google_compute_subnetwork.proxy_only.id } HTTP Target Proxy resource \u0026#34;google_compute_region_target_http_proxy\u0026#34; \u0026#34;default\u0026#34; { name = \u0026#34;http-lb-proxy\u0026#34; region = \u0026#34;us-central1\u0026#34; url_map = google_compute_region_url_map.default.id } URL Map This is where we implement the traffic split. We use weighted_backend_services to control the distribution of requests. By setting the weight for stable to 80 and canary to 20, we ensure that 80% of the incoming traffic is directed to the stable version, while 20% goes to the canary version. This allows us to gradually expose users to the new version while maintaining the majority of traffic on the proven, stable version.\nresource \u0026#34;google_compute_region_url_map\u0026#34; \u0026#34;default\u0026#34; { name = \u0026#34;url-map\u0026#34; default_service = google_compute_region_backend_service.stable.id region = \u0026#34;us-central1\u0026#34; host_rule { hosts = [\u0026#34;*\u0026#34;] path_matcher = \u0026#34;allpaths\u0026#34; } path_matcher { name = \u0026#34;allpaths\u0026#34; default_service = google_compute_region_backend_service.stable.id route_rules { priority = 1 match_rules { prefix_match = \u0026#34;/\u0026#34; } route_action { weighted_backend_services { backend_service = google_compute_region_backend_service.stable.id weight = 80 } weighted_backend_services { backend_service = google_compute_region_backend_service.canary.id weight = 20 } } } } } Canary Instance Template resource \u0026#34;google_compute_instance_template\u0026#34; \u0026#34;canary\u0026#34; { name = \u0026#34;canary-template\u0026#34; machine_type = \u0026#34;e2-micro\u0026#34; disk { source_image = \u0026#34;debian-cloud/debian-11\u0026#34; auto_delete = true boot = true } network_interface { network = \u0026#34;default\u0026#34; access_config {} } metadata_startup_script = \u0026lt;\u0026lt;-EOF #!/bin/bash apt-get update apt-get install -y nginx echo \u0026#34;Canary Version\u0026#34; \u0026gt; /var/www/html/index.html EOF tags = [\u0026#34;http-server\u0026#34;] } Canary Managed Instance Group resource \u0026#34;google_compute_region_instance_group_manager\u0026#34; \u0026#34;canary\u0026#34; { name = \u0026#34;canary-mig\u0026#34; base_instance_name = \u0026#34;canary\u0026#34; region = \u0026#34;us-central1\u0026#34; target_size = 1 version { instance_template = google_compute_instance_template.canary.id } named_port { name = \u0026#34;http\u0026#34; port = 80 } } Stable Instance Template resource \u0026#34;google_compute_instance_template\u0026#34; \u0026#34;stable\u0026#34; { name = \u0026#34;stable-template\u0026#34; machine_type = \u0026#34;e2-micro\u0026#34; disk { source_image = \u0026#34;debian-cloud/debian-11\u0026#34; auto_delete = true boot = true } network_interface { network = \u0026#34;default\u0026#34; access_config {} } metadata_startup_script = \u0026lt;\u0026lt;-EOF #!/bin/bash apt-get update apt-get install -y nginx echo \u0026#34;Stable Version\u0026#34; \u0026gt; /var/www/html/index.html EOF tags = [\u0026#34;http-server\u0026#34;] } Stable Managed Instance Group resource \u0026#34;google_compute_region_instance_group_manager\u0026#34; \u0026#34;stable\u0026#34; { name = \u0026#34;stable-mig\u0026#34; base_instance_name = \u0026#34;stable\u0026#34; region = \u0026#34;us-central1\u0026#34; target_size = 2 version { instance_template = google_compute_instance_template.stable.id } named_port { name = \u0026#34;http\u0026#34; port = 80 } } DRAFT DRAFT DRAFT DRAFT DRAFT DRAFT\n","permalink":"https://riza.netlify.app/blog/canary-deployment-gcp/","summary":"Introduction Let\u0026rsquo;s face it, pushing new code to production can be nerve-wracking. One tiny bug and suddenly you\u0026rsquo;re putting out fires instead of sipping your well-earned coffee. That\u0026rsquo;s where canary deployments come in handy. They let you dip your toes in the water before diving in headfirst. In this article, we\u0026rsquo;re going to walk through setting up a canary deployment system on Google Cloud Platform (GCP) using Terraform. We\u0026rsquo;ll be working with regional managed instance groups, load balancers, and some traffic-splitting magic to get a solid canary setup going.","title":"[DRAFT] Building a Canary Deployment Infrastructure in GCP"},{"content":"Introduction As a platform team, our goal is to empower developers with easy-to-use tools for creating infrastructure. Our Internal Developer Portal offers a suite of tools to enhance developers\u0026rsquo; daily workflows. In this article, we\u0026rsquo;ll focus on a key feature: onboarding an application with a single click.\nBackground Our cloud provider is Google Cloud Platform (GCP) We use Terraform for infrastructure configuration This article focuses on VM-based applications While we won\u0026rsquo;t dive deep into the Terraform details in this article, it\u0026rsquo;s important to note that we create infrastructure by defining Terraform resources.\nBase Infrastructure Components For each application, we define the following base infrastructure:\nService Account Instance Template Instance Group Backend Service URL Map The Onboarding Process Step 1: User Input Users begin by filling out a form in our Developer Portal, providing:\nApplication name Instance template configurations: Machine type Disk size Ubuntu image version Instance group configurations: Maximum replicas Minimum replicas Step 2: Pipeline Trigger Upon form submission, we trigger a GitLab pipeline using GitLab\u0026rsquo;s trigger API. This process involves the following:\nPrerequisites Before triggering the pipeline, we need to generate a trigger token. This can be done using the GitLab API for creating trigger token. For more information, refer to the GitLab documentation on creating trigger tokens.\nTriggering the Pipeline We use a cURL command to trigger the pipeline:\ncurl --request POST \\ \u0026#34;https://gitlab.example.com/api/v4/projects/\u0026lt;project_id\u0026gt;/trigger/pipeline?token=\u0026lt;token\u0026gt;\u0026amp;ref=\u0026lt;ref_name\u0026gt;\u0026#34; Where:\n\u0026lt;token\u0026gt; is your trigger token \u0026lt;ref_name\u0026gt; is a branch or tag name (e.g., main) \u0026lt;project_id\u0026gt; is your project ID (e.g., 123456), which can be found on the repository page Passing User Input We pass the user-provided values in the request body as a JSON object. Our script later parses this JSON to extract the necessary information.\nPipeline Actions Once triggered, the pipeline:\nManages the state of base infrastructure creation Raises a Merge Request (MR) to our Infrastructure as Code (IaC) repository Step 3: Automated Resource Creation Our pipeline includes a script that:\nRaises the MR to GitLab Creates resources in a defined sequence Automates the entire process The script creates the resources in sequence, and the entire process is automated within the pipeline.\nDRAFT DRAFT DRAFT DRAFT DRAFT DRAFT\n","permalink":"https://riza.netlify.app/blog/gitlab-pipeline-infra/","summary":"Introduction As a platform team, our goal is to empower developers with easy-to-use tools for creating infrastructure. Our Internal Developer Portal offers a suite of tools to enhance developers\u0026rsquo; daily workflows. In this article, we\u0026rsquo;ll focus on a key feature: onboarding an application with a single click.\nBackground Our cloud provider is Google Cloud Platform (GCP) We use Terraform for infrastructure configuration This article focuses on VM-based applications While we won\u0026rsquo;t dive deep into the Terraform details in this article, it\u0026rsquo;s important to note that we create infrastructure by defining Terraform resources.","title":"[DRAFT] Streamlining Application Infrastructure: A Platform Team's Approach"},{"content":"","permalink":"https://riza.netlify.app/blog/dev-platform/","summary":"","title":"[DRAFT] Optimizing Developer Workflows: Our Internal Developer Platform"},{"content":"","permalink":"https://riza.netlify.app/blog/iac/","summary":"","title":"[DRAFT] Streamlining Infrastructure as Code with Terraform, Atlantis, and Terragrunt"}]